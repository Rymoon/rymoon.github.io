{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction to Machine Learning* Algorithms and Realizations 2\n",
    "\n",
    "## By Jiaheng Cui\n",
    ">In this chapter, we'll focus on Newton's Method and DFP Quasi-Newton Method. If you are interested, you can also see how Zhihao Lyu used Newton's Method to solve the MLE of a Gamma distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Newton's Method\n",
    "### (1) Basic thoughts\n",
    "We introduced that gradient descent is a first-order optimization algorithm for finding a local minimum of a differentiable function in the last chapter. Now we introduce a second-order method, Newton's Method, or the Newton-Raphson Method.\n",
    "\n",
    "We want to solve the following optimization problem:\n",
    "$$ \\mathop{\\min}\\limits_{x \\in R^{n \\times 1}} f(x)$$\n",
    "\n",
    "Let $f(x)$ be **twice** differentiable, and $ x^{*} $ be the corresponding local minimum of $f(x)$.\n",
    "\n",
    "The 2nd-order Taylor expansion of $f(x)$ is $f(x+\\Delta x) = f(x) + g(x)\\Delta x + \\frac{1}{2} \\Delta x^T H(x)\\Delta x+ O(\\parallel \\Delta x \\parallel^3)$, where $g(x) = \\nabla f(x)$ is the gradient of $f$ at point $x$, $H(x) = \\nabla^2 f(x)$ is the Hessian matrix of $f$ at point $x$. If we ignore the $O(\\parallel \\Delta x \\parallel^3)$ term, then $f(x+\\Delta x) \\approx f(x) + \\nabla f(x)\\Delta x + \\frac{1}{2} \\Delta x^T H(x)\\Delta x$, we can use this formula to derive an iterative method to solve $x^*$.\n",
    "\n",
    "We'll choose a starting point $x_0$ and use the following equation to implement the iteration:\n",
    "$x_{k+1} = x_k - H(x)^{-1} g_k = x_k - \\left[ \\nabla^2 f(x) \\right]^{-1} \\nabla f(x)$, $k = 0,1,2,...$\n",
    "\n",
    "Again, if $f(x)$ is convex, then we have only one minimum point $x^{*}$, so the minimum we derived from gradient descent is $x^{*}$ itself (assuming there were no error).\n",
    "\n",
    "### (2) Algorithm\n",
    "Input: target function $f(x)$, gradient function $g(x) = \\nabla f(x)$, Hessian matrix $H(x) = \\nabla^2 f(x)$, tolerance $\\epsilon$, starting point $x_0 \\in R^{n \\times 1}$, max iteration number $k_{max}$.\n",
    "\n",
    "Output: a minimum point of $f(x)$, $x^{*}$.\n",
    "\n",
    "Step1: load $x_0$, and set $k = 0$.\n",
    "\n",
    "Step2: calculate $f(x_k)$\n",
    "\n",
    "Step3: calculate $g_k = g(x_k)$, if $\\parallel g_k\\parallel \\lt \\epsilon$, stop iteration and let $x^* = x_k$; otherwise, let $p_k = -g_k$.\n",
    "\n",
    "Step4: calculate $H_k = H(x_k)$, if $H_k$ is non-invertible, print \"Hessian matrix non-invertible, calculation failed.\", stop iteration; otherwise, calculate $H_{k}^{-1} = H^{-1}(x_k)$.\n",
    "\n",
    "Step4: let $x_{k+1} = x_k + H_k^{-1} p_k$, calculate $f(x_{k+1})$.\n",
    "\n",
    "Step5: if $\\parallel f(x_{k+1}) - f(x_{k})\\parallel \\lt \\epsilon$ or $\\parallel x_{k+1} - x_{k}\\parallel \\lt \\epsilon$, stop iteration and let $x^* = x_{k+1}$; otherwise, let $k = k+1$.\n",
    "\n",
    "Step6: if $k = k_{max}$, the algorithm doesn't converge, print \"Does not converge, calculation failed.\"; otherwise, return to Step3.\n",
    "\n",
    "### (3) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(f_x, g_x, H_x, x_0, eps = 1e-5, k_max = 1e5):\n",
    "    x_k = x_0\n",
    "    k = 0\n",
    "    \n",
    "    for k in range(k_max + 1):\n",
    "        g_k = g_x(x_k)\n",
    "        \n",
    "        if(np.linalg.norm(g_k) < eps):\n",
    "            return (x_k,k)\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            p_k = - g_k\n",
    "            H_k = H_x(x_k)\n",
    "            if(np.linalg.det(H_k) == 0):\n",
    "                return \"Hessian matrix non-invertible, calculation failed.\"\n",
    "            \n",
    "            else:\n",
    "                x_k_new = x_k + np.linalg.inv(H_k).dot(p_k)\n",
    "                if(np.linalg.norm(f_x(x_k_new) - f_x(x_k)) < eps or np.linalg.norm(x_k_new - x_k) < eps):\n",
    "                    return (x_k_new,k)\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    x_k = x_k_new\n",
    "                    if(k == k_max):\n",
    "                        return \"Does not converge, calculation failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(X) = (x_1 − x_2)^3 + (x_1 + 3x_2)^2$.\n",
    "\n",
    "Then $g(x) = \\nabla f(x) = (3(x_1 - x_2)^2 + 2(x_1 + 3x_2), -3(x_1 - x_2)^2 + 6(x_1 + 3x_2))^T$.\n",
    "\n",
    "And $H(x) = \\nabla^2 f(x) = \\left[\\begin{matrix} 6(x_1 - x_2) + 2 & -6(x_1 - x_2) + 6\\\\ -6(x_1 - x_2) + 6 & 6(x_1 - x_2) + 18\\\\ \\end{matrix}\\right]$.\n",
    "\n",
    "Let the starting point be $x_0 = (1,2)^T$, we'll use Newton's method to find a minimum point of $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.00585938,  0.00195313]), 6)\n"
     ]
    }
   ],
   "source": [
    "x_0 = np.array([1,2])\n",
    "\n",
    "def f_test(x):\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    return ((x_1 - x_2) ** 3 + (x_1 + 3*x_2) ** 2)\n",
    "\n",
    "def g_test(x):\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    return np.array([3*(x_1 - x_2) ** 2 + 2*(x_1 + 3*x_2), -3*(x_1 - x_2) ** 2 + 6*(x_1 + 3*x_2)])\n",
    "\n",
    "def H_test(x):\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    return np.array([[6*(x_1 - x_2) + 2, -6*(x_1 - x_2) + 6], [-6*(x_1 - x_2) + 6, 6*(x_1 - x_2) + 18]])\n",
    "\n",
    "print(Newton(f_test, g_test, H_test, x_0, 1e-5, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Use Newton's Method to solve the MLE of gamma distribution\n",
    "\n",
    "Please refer to the passage written by Zhihao Lyu:\n",
    "\n",
    "https://github.com/jimcui3/Introduction-to-Machine-Learning/blob/main/2020-10-26-MLE%20with%20Newton%20Raphson.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Quasi-Newton Method\n",
    "### (1) Basic thoughts\n",
    "Note that during the iteration, $H(x_k)$ is not always invertible. If there exists a $k$ s.t. $det(H(x_k)) = 0$, we cannot perform the Newton's method iteration. Thus there are many thoughts to use a series of invertible matrices to estimate **the inverse of the Hessian matrix**, we still denote these estimators as $\\left\\{ H_k \\right\\}$. These methods are called Quasi-Newton Methods. Everytime we'll use $H_k$ compute $x_{k+1}$, , then if didn't reach convergence, we'll get $H_{k+1}$ by a certain formula concerning $x_k$, $g_k$ and $H_k$. Then use $x_{k+1}$, $g_{k+1}$ and $H_{k+1}$ to compute $x_{k+2}$.\n",
    "\n",
    "The most common Quasi-Newton Methods include DFP Quasi-Newton Method and BFGS Quasi-Newton Method. We'll only talk about DFP Quasi-Newton Method, those who are interested in BFGS Quasi-Newton Method can look it up here:\n",
    "\n",
    "https://en.wikipedia.org/wiki/BFGS_method\n",
    "\n",
    "### (2) DFP Quasi-Newton Method\n",
    "Let $H_0 = I_n$, where $I_n$ is the identity matrix of order $n$.\n",
    "\n",
    "Let $\\Delta x_k = x_{k+1} - x_k$, $\\Delta g_k = g_{k+1} - g_k$. Note that we do this step after we got $x_{k+1}$ (and thus $g_{k+1}$) from the iteration formula and only when we won't stop at $x_{k+1}$.\n",
    "\n",
    "The iterative formula of $H_k$ is listed as follows:\n",
    "$$H_{k+1} = H_k + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta x_k^T \\Delta g_k} - \\frac{H_k \\Delta g_k \\Delta g_k^T H_k}{\\Delta g_k^T H_k \\Delta g_k}$$\n",
    "\n",
    "Note: It may be a little difficult to understand the double iteration loops and their sequential orders, if so, please review the following algorithm carefully and make sure you understand it clearly!\n",
    "\n",
    "### (3) Algorithm\n",
    "Input: target function $f(x)$, gradient function $g(x) = \\nabla f(x)$, tolerance $\\epsilon$, starting point $x_0 \\in R^{n \\times 1}$, max iteration number $k_{max}$.\n",
    "\n",
    "Output: a minimum point of $f(x)$, $x^{*}$.\n",
    "\n",
    "Step1: load $x_0$, and set $k = 0$, $H_0 = I$.\n",
    "\n",
    "Step2: calculate $f(x_k)$\n",
    "\n",
    "Step3: calculate $g_k = g(x_k)$, if $\\parallel g_k\\parallel \\lt \\epsilon$, stop iteration and let $x^* = x_k$; otherwise, continue.\n",
    "\n",
    "Step4: let $x_{k+1} = x_k - H_k g_k$, calculate $f(x_{k+1})$.\n",
    "\n",
    "Step5: if $\\parallel f(x_{k+1}) - f(x_{k})\\parallel \\lt \\epsilon$ or $\\parallel x_{k+1} - x_{k}\\parallel \\lt \\epsilon$, stop iteration and let $x^* = x_{k+1}$; otherwise, continue.\n",
    "\n",
    "Step6: calculate $g_{k+1} = g(x_{k+1})$, if $\\parallel g_{k+1}\\parallel \\lt \\epsilon$, stop iteration and let $x^* = x_{l+1}; otherwise, $calculate $\\Delta x_k = x_{k+1} - x_k$, $\\Delta g_k = g_{k+1} - g_k$.\n",
    "\n",
    "Step7: let $H_{k+1} = H_k + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta x_k^T \\Delta g_k} - \\frac{H_k \\Delta g_k \\Delta g_k^T H_k}{\\Delta g_k^T H_k \\Delta g_k}$, let $k = k+1$\n",
    "\n",
    "Step8: if $k = k_{max}$, the algorithm doesn't converge, print \"Does not converge, calculation failed.\"; otherwise, return to Step3.\n",
    "\n",
    "### (4) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFP_Quasi_Newton(f_x, g_x, x_0, eps = 1e-5, k_max = 1e5):\n",
    "    x_k = x_0\n",
    "    H_k = np.identity(x_0.shape[0])\n",
    "    k = 0\n",
    "    \n",
    "    for k in range(k_max + 1):\n",
    "        g_k = g_x(x_k)\n",
    "        \n",
    "        if(np.linalg.norm(g_k) < eps):\n",
    "            return (x_k,k)\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            x_k_new = x_k - H_k.dot(g_k)\n",
    "            \n",
    "            if(np.linalg.norm(f_x(x_k_new) - f_x(x_k)) < eps or np.linalg.norm(x_k_new - x_k) < eps):\n",
    "                return (x_k_new, k)\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                g_k_new = g_x(x_k_new)\n",
    "        \n",
    "                if(np.linalg.norm(g_k_new) < eps):\n",
    "                    return (x_k_new, k)\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    delta_x_k = x_k_new - x_k\n",
    "                    delta_g_k = g_k_new - g_k\n",
    "                    H_k = H_k + np.outer(delta_x_k, delta_x_k) / delta_x_k.T.dot(delta_g_k) - H_k.dot(np.outer(delta_g_k, delta_g_k)).dot(H_k) / delta_g_k.T.dot(H_k).dot(delta_g_k)\n",
    "                    x_k = x_k_new\n",
    "\n",
    "                if(k == k_max):\n",
    "                    return \"Does not converge, calculation failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(X) = (4 − x_2)^3 + (x_1 + 4x_2)^2$.\n",
    "\n",
    "Then $g(x) = \\nabla f(x) = (2(x_1 + 4x_2), -3(4 - x_2)^2 + 8(x_1 + 4x_2))^T$.\n",
    "\n",
    "Let the starting point be $x_0 = (2,1)^T$, we'll use Newton's method and DFP Quasi-Newton method to find a minimum point of $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-15.9765625 ,   3.99414062]), 8)\n",
      "(array([-15.9630806,   3.9907569]), 14)\n"
     ]
    }
   ],
   "source": [
    "x_0 = np.array([2,1])\n",
    "\n",
    "def f_test(x):\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    return ((4 - x_2) ** 3 + (x_1 + 4*x_2) ** 2)\n",
    "\n",
    "def g_test(x):\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    return np.array([2*(x_1 + 4*x_2), -3*(4 - x_2) ** 2 + 8*(x_1 + 4*x_2)])\n",
    "\n",
    "def H_test(x):\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    return np.array([[2, 8], [8, 6*(4 - x_2) + 32]])\n",
    "\n",
    "print(Newton(f_test, g_test, H_test, x_0, 1e-5, 1000))\n",
    "print(DFP_Quasi_Newton(f_test, g_test, x_0, 1e-5, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using DFP Quasi-Newton method is slower than regular Newton's method, however it's more stable.\n",
    "\n",
    "Note: (-16,4) is only a local minimum ($gradient = 0$). $f(x)$ has no global minimum point, since when $x_1 = -4x_2$, and $x_2 \\rightarrow + \\infty$, $f(x) \\rightarrow - \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1.统计学习方法（第2版）- 李航\n",
    "\n",
    "2.最优化方法 - 杨庆之\n",
    "\n",
    "3.Hessian Matrix - Wikipedia\n",
    "\n",
    "4.Newton's method - Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
