<!DOCTYPE html>
<html lang="en-US">
  <head>
     <meta charset="utf-8">
     
         <script type="text/x-mathjax-config">
 MathJax.Hub.Config({
     TeX: {
         equationNumbers: {
             autoNumber: "AMS"
         }
     },
     tex2jax: {
         inlineMath: [ ['$','$'] ],
         processEscapes: true,
     }
 });
 </script>
 <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
 </script>

     


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A course in pytorch 2 | Rymoon’s Website</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="A course in pytorch 2" />
<meta name="author" content="vjy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Pytorch 入门 本文翻译自pytorch官方教程DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ，共四篇。 这是第二篇AUTOGRAD: AUTOMATIC DIFFERENTIATION." />
<meta property="og:description" content="Pytorch 入门 本文翻译自pytorch官方教程DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ，共四篇。 这是第二篇AUTOGRAD: AUTOMATIC DIFFERENTIATION." />
<link rel="canonical" href="http://localhost:4000/2020/09/26/A-Course-in-Pytorch-2.html" />
<meta property="og:url" content="http://localhost:4000/2020/09/26/A-Course-in-Pytorch-2.html" />
<meta property="og:site_name" content="Rymoon’s Website" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-26T00:00:00+08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/2020/09/26/A-Course-in-Pytorch-2.html","headline":"A course in pytorch 2","dateModified":"2020-09-26T00:00:00+08:00","datePublished":"2020-09-26T00:00:00+08:00","author":{"@type":"Person","name":"vjy"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/09/26/A-Course-in-Pytorch-2.html"},"description":"Pytorch 入门 本文翻译自pytorch官方教程DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ，共四篇。 这是第二篇AUTOGRAD: AUTOMATIC DIFFERENTIATION.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=bfaedc29297281d39ebd0fa1c434be3d3f948736">
  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">A course in pytorch 2</h1>
      <h2 class="project-tagline">A humble website.</h2>
      
        <a href="https://github.com/Rymoon/rymoon.github.io" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1>A course in pytorch 2</h1>

<p>
  26 Sep 2020
  
  
    - <a href="/authors/vjy.html">张佳毅</a>
  
</p>

<h1 id="pytorch-入门">Pytorch 入门</h1>

<blockquote>
  <p>本文翻译自pytorch官方教程<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ</a>，共四篇。
这是第二篇AUTOGRAD: AUTOMATIC DIFFERENTIATION.</p>
</blockquote>

<!-- more -->
<h1 id="autograd-自动微分">Autograd: 自动微分</h1>

<p>PyTorch 中的所有神经网络的核心就是 <code class="language-plaintext highlighter-rouge">autograd</code>包。我们先简单浏览，然后我们将训练我们的第一个神经网络。</p>

<p><code class="language-plaintext highlighter-rouge">autograd</code>包为所有 tensors 的操作提供自动微分功能。它是一个在运行式才被定义的框架， 意味着反向传播只有在代码运行时才会计算， 每次迭代时有可能不同。</p>

<p>让我们用一些简单的例子来说明这一点。</p>

<h2 id="tensor">Tensor</h2>

<p><code class="language-plaintext highlighter-rouge">torch.Tensor</code> 是 PyTorch 中的核心类。 如果你将它的属性 <code class="language-plaintext highlighter-rouge">.requires_grad</code> 设置为 <code class="language-plaintext highlighter-rouge">True</code>，它就会开始追踪 在其上的所有操作。当你完成你的计算你可以调用 <code class="language-plaintext highlighter-rouge">backward()</code>  然后所有的梯度都将自动计算。tensor 的梯度都会被收集到 <code class="language-plaintext highlighter-rouge">.grad</code> 属性。</p>

<p>要停止 tensor 追踪历史， 你可以调用 <code class="language-plaintext highlighter-rouge">.detach()</code> ，它将与其计算历史记录分离，并防止将来的计算被追踪。</p>

<p>为了防止追踪历史 (使用内存)，你可以代码块放入 <code class="language-plaintext highlighter-rouge">withtorch.no_grad()</code>中：这中方法在我们计算一个带有<code class="language-plaintext highlighter-rouge">requires_grad=True</code> 参数却又不需要梯度的模型时会非常有用。</p>

<p>另外一个对 <code class="language-plaintext highlighter-rouge">autocrat</code> 实现非常重要的类是 <code class="language-plaintext highlighter-rouge">Function</code></p>

<p><code class="language-plaintext highlighter-rouge">Tensor</code> 和 <code class="language-plaintext highlighter-rouge">Function</code> 是相互连接的并且建立了一个记录完整计算历史的无环图。每个 tensor 都拥有 <code class="language-plaintext highlighter-rouge">.grad_fn</code> 属性保存着创建 tensor 的 Function 的引用，(如果用户自己创建张量，则 <code class="language-plaintext highlighter-rouge">grad_fn</code> 是 <code class="language-plaintext highlighter-rouge">None</code>)。</p>

<p>如果你想要计算导数，你可以在 <code class="language-plaintext highlighter-rouge">tensor</code> 上调用 <code class="language-plaintext highlighter-rouge">.backward()</code>。如果 <code class="language-plaintext highlighter-rouge">Tensor</code> 是一个标量 (例如，它只含有一个元素)，你不需要为 <code class="language-plaintext highlighter-rouge">.backward()</code> 指定任何参数，然而如果它拥有多个元素，则需要指定 <code class="language-plaintext highlighter-rouge">gradient</code> 参数来指定 tensor 的形状。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
</code></pre></div></div>
<p>创建 tensor 并设置 <code class="language-plaintext highlighter-rouge">requires_grad=True</code> 来追踪计算</p>
<pre><code class="language-python3">x = torch.ones(2, 2, requires_grad=True)
print(x)
</code></pre>
<p>进行 tensor 运算</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">y</code> 是运算的结果， 所以它拥有 <code class="language-plaintext highlighter-rouge">grad_fn</code>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div></div>

<p>对 <code class="language-plaintext highlighter-rouge">y</code> 进行更多运算</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">.requires_grad_( ... )</code> 会改变张量的 <code class="language-plaintext highlighter-rouge">requires_grad</code> 标记。如果没有提供相应的参数，输入的标记默认为 <code class="language-plaintext highlighter-rouge">False</code>。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">a</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="梯度">梯度</h2>
<p>我们开始进行反向传播。由于 <code class="language-plaintext highlighter-rouge">out</code> 包含一个标量， <code class="language-plaintext highlighter-rouge">out.backward()</code> 与 <code class="language-plaintext highlighter-rouge">out.backward(torch.tensor(1.))</code> 等价</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>
<p>打印梯度 d(out)/dx</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>
<p>你会得到一个元素全部为 <code class="language-plaintext highlighter-rouge">4.5</code> 的矩阵。我们将 <code class="language-plaintext highlighter-rouge">out</code> 记为 <em>Tensor</em> “$o$”。我们得到 $o = \frac{1}{4}\sum_i z_i$,  $z_i = 3(x_i+2)^2$  and  $z_i\bigr\rvert_{x_i=1} = 27$。因此， $\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)$，$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$.</p>

<p>数学上， 如果有一个向量值函数 $\vec{y}=f(\vec{x})$, 那么 $\vec{y}$ 关于 $\vec{x}$ 的梯度是一个雅克比矩阵：</p>

\[\begin{align}J=\left(\begin{array}{ccc}
   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\
   \vdots &amp; \ddots &amp; \vdots\\
   \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}
   \end{array}\right)\end{align}\]

<p>一般来说， <code class="language-plaintext highlighter-rouge">torch.autograd</code> 是一个计算 vector-Jacobian product(雅克比矩阵与向量的乘积)的工具。 给定任意向量 $v=\left(\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \cdots v_{m}\end{array}\right)^{T}$, 计算乘积 $v^{T}\cdot J$. 如果 $v$ 恰好是标量函数 $l=g\left(\vec{y}\right)$ 的梯度， 则 $v=\left( \frac{\partial l}{\partial y_{1}} \cdots \frac{\partial l}{\partial y_{m}}\right)^{T}$, 然后通过链式法则，the vector-Jacobian product 就是 $l$ 关于 $\vec{x}$的梯度：
\(\begin{align}J^{T}\cdot v=\left(\begin{array}{ccc}
   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}\\
   \vdots &amp; \ddots &amp; \vdots\\
   \frac{\partial y_{1}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}
   \end{array}\right)\left(\begin{array}{c}
   \frac{\partial l}{\partial y_{1}}\\
   \vdots\\
   \frac{\partial l}{\partial y_{m}}
   \end{array}\right)=\left(\begin{array}{c}
   \frac{\partial l}{\partial x_{1}}\\
   \vdots\\
   \frac{\partial l}{\partial x_{n}}
   \end{array}\right)\end{align}\)</p>

<p>(注意 $v^{T}\cdot J$ 得到一个行向量， 它也可以通过 $J^{T}\cdot v$ 得到一个列向量.)</p>

<p>vector-Jacobian product 的特性使得一个非标量输出的外部梯度更容易被输入模型。</p>

<p>现在我们来看一个 vertor-Jacobian product 的例子</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="k">while</span> <span class="n">y</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>

<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>在这个例子中 <code class="language-plaintext highlighter-rouge">y</code> 不再是标量。<code class="language-plaintext highlighter-rouge">torch.autograd</code> 不能直接计算完整的雅克比矩阵, 但是如果我们只想要 vector-Jacobian product，只需要将向量传给 <code class="language-plaintext highlighter-rouge">backward</code> 作为参数。</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>
<p>你可以通过将带有 <code class="language-plaintext highlighter-rouge">.requires_grad=True</code> 的 tensor 放入<code class="language-plaintext highlighter-rouge">with torch.no_grad()</code>代码块中来防止 autograd 追踪历史：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div>
<p>或者通过使用 <code class="language-plaintext highlighter-rouge">.detach()</code> 来获得一个新的具有同样内容但不需要梯度的 tensor：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nb">all</span><span class="p">())</span>
</code></pre></div></div>
<p><strong>Read Later</strong>
有关 <code class="language-plaintext highlighter-rouge">autograd.Function</code> 的文档 [文档]：https://pytorch.org/docs/stable/autograd.html#function</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Rymoon/rymoon.github.io">rymoon.github.io</a> is maintained by <a href="https://github.com/Rymoon">Rymoon</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
  </body>
</html>
