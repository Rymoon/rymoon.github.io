<!DOCTYPE html>
<html lang="en-US">
  <head>


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A course in pytorch 3 | Rymoon’s Website</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="A course in pytorch 3" />
<meta name="author" content="rym" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Pytorch 入门 本文翻译自pytorch官方教程DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ，共四篇。 这是第三篇NEURAL NETWORKS." />
<meta property="og:description" content="Pytorch 入门 本文翻译自pytorch官方教程DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ，共四篇。 这是第三篇NEURAL NETWORKS." />
<link rel="canonical" href="http://localhost:4000/2020/09/26/A-Course-in-Pytorch-3.html" />
<meta property="og:url" content="http://localhost:4000/2020/09/26/A-Course-in-Pytorch-3.html" />
<meta property="og:site_name" content="Rymoon’s Website" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-26T00:00:00+08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"A course in pytorch 3","dateModified":"2020-09-26T00:00:00+08:00","datePublished":"2020-09-26T00:00:00+08:00","url":"http://localhost:4000/2020/09/26/A-Course-in-Pytorch-3.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/09/26/A-Course-in-Pytorch-3.html"},"author":{"@type":"Person","name":"rym"},"description":"Pytorch 入门 本文翻译自pytorch官方教程DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ，共四篇。 这是第三篇NEURAL NETWORKS.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=8f39b20fb92325a008f3e9637741fded350ffffd">
  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">A course in pytorch 3</h1>
      <h2 class="project-tagline">A humble website.</h2>
      
        <a href="https://github.com/Rymoon/rymoon.github.io" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1>A course in pytorch 3</h1>

<p>
  26 Sep 2020
  
  
    - <a href="/authors/rym.html">任雨濛</a>
  
</p>

<h1 id="pytorch-入门">Pytorch 入门</h1>

<blockquote>
  <p>本文翻译自pytorch官方教程<a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ</a>，共四篇。
这是第三篇NEURAL NETWORKS.</p>
</blockquote>

<!-- more -->

<h1 id="神经网络">神经网络</h1>

<p>用torch.nn搭建神经网络。</p>

<p>nn依赖自动微分(autograd)来搭建模型，并求梯度。一个nn.Module包括网络层(layer)，和（输入到输出的）前向方法(forward)。</p>

<p>举个例子，一个图像分类网络convnet。这是一个简单的前馈网络。输入(input)依次经过各层，到达输出。一个典型的训练过程如下：</p>

<p><img src="http://localhost:4000/assets/image/2020-09-26-A-Course-in-Pytorch-3/convnet.png" alt="convnet" /></p>

<ul>
  <li>用一些可学习参数定义网络；</li>
  <li>在输入数据集上迭代；</li>
  <li>使输入通过网络；</li>
  <li>计算损失(loss)（输出与正确相差多远）；</li>
  <li>反向传播梯度(gradient)至各个参数；</li>
  <li>更新网络参数(weight)，比如：
<code class="language-plaintext highlighter-rouge">weight = weight - learning_rate * gradient</code></li>
</ul>

<h2 id="定义网络">定义网络</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># 1 input image channel, 6 output channels, 3x3 square convolution
</span>        <span class="c1"># kernel
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c1"># an affine operation: y = Wx + b
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">6</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>  <span class="c1"># 6*6 from image dimension
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Max pooling over a (2, 2) window
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="c1"># If the size is a square you can only specify a single number
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_flat_features</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">num_flat_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># all dimensions except the batch dimension
</span>        <span class="n">num_features</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">size</span><span class="p">:</span>
            <span class="n">num_features</span> <span class="o">*=</span> <span class="n">s</span>
        <span class="k">return</span> <span class="n">num_features</span>


<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-bat highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">Out</span>:

<span class="kd">Net</span><span class="o">(</span>
  <span class="o">(</span><span class="kd">conv1</span><span class="o">)</span>: <span class="kd">Conv2d</span><span class="o">(</span><span class="m">1</span><span class="o">,</span> <span class="m">6</span><span class="o">,</span> <span class="kd">kernel_size</span><span class="o">=(</span><span class="m">3</span><span class="o">,</span> <span class="m">3</span><span class="o">),</span> <span class="kd">stride</span><span class="o">=(</span><span class="m">1</span><span class="o">,</span> <span class="m">1</span><span class="o">))</span>
  <span class="o">(</span><span class="kd">conv2</span><span class="o">)</span>: <span class="kd">Conv2d</span><span class="o">(</span><span class="m">6</span><span class="o">,</span> <span class="m">16</span><span class="o">,</span> <span class="kd">kernel_size</span><span class="o">=(</span><span class="m">3</span><span class="o">,</span> <span class="m">3</span><span class="o">),</span> <span class="kd">stride</span><span class="o">=(</span><span class="m">1</span><span class="o">,</span> <span class="m">1</span><span class="o">))</span>
  <span class="o">(</span><span class="kd">fc1</span><span class="o">)</span>: <span class="kd">Linear</span><span class="o">(</span><span class="kd">in_features</span><span class="o">=</span><span class="m">576</span><span class="o">,</span> <span class="kd">out_features</span><span class="o">=</span><span class="m">120</span><span class="o">,</span> <span class="kd">bias</span><span class="o">=</span><span class="kd">True</span><span class="o">)</span>
  <span class="o">(</span><span class="kd">fc2</span><span class="o">)</span>: <span class="kd">Linear</span><span class="o">(</span><span class="kd">in_features</span><span class="o">=</span><span class="m">120</span><span class="o">,</span> <span class="kd">out_features</span><span class="o">=</span><span class="m">84</span><span class="o">,</span> <span class="kd">bias</span><span class="o">=</span><span class="kd">True</span><span class="o">)</span>
  <span class="o">(</span><span class="kd">fc3</span><span class="o">)</span>: <span class="kd">Linear</span><span class="o">(</span><span class="kd">in_features</span><span class="o">=</span><span class="m">84</span><span class="o">,</span> <span class="kd">out_features</span><span class="o">=</span><span class="m">10</span><span class="o">,</span> <span class="kd">bias</span><span class="o">=</span><span class="kd">True</span><span class="o">)</span>
<span class="o">)</span>
</code></pre></div></div>

<p>我们定义了一个前向函数，后向(backward)函数（用于梯度计算）由pytorch的自动微分自动生成。在前向函数可以使用张量运算。</p>

<p>net.parameters()返回模型的可学习参数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># conv1's .weight
</span></code></pre></div></div>

<div class="language-bat highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">Out</span>:

<span class="m">10</span>
<span class="kd">torch</span>.Size<span class="o">([</span><span class="m">6</span><span class="o">,</span> <span class="m">1</span><span class="o">,</span> <span class="m">3</span><span class="o">,</span> <span class="m">3</span><span class="o">])</span>
</code></pre></div></div>

<p>随机的32x32输入。</p>

<blockquote>
  <p><em>Note</em><br />
这个网络(LeNet)的输入应为32x32. 在 MNIST 数据集上使用网络, 需要缩放至32x32.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">Out</span><span class="p">:</span>

<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1074</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0430</span><span class="p">,</span>  <span class="mf">0.0831</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1322</span><span class="p">,</span>  <span class="mf">0.0159</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0842</span><span class="p">,</span>  <span class="mf">0.1799</span><span class="p">,</span>  <span class="mf">0.1090</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.0416</span><span class="p">,</span>  <span class="mf">0.0307</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div>

<p>将所有参数的梯度缓冲区(buffer)置零，并且从随机的梯度开始反向传播。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

</code></pre></div></div>

<blockquote>
  <p><em>NOTE</em><br />
torch.nn只支持mini-batches. torch.nn 包只支持样本的mini-batch作为输入(如：8x32x32)，而不是一个单独的输入（如：32x32）。<br />
例如，nn.Conv2d的输入是4维张量nSamples x nChannels x Height x Width。<br />
如果只有一个样本，用<code class="language-plaintext highlighter-rouge">input.unsqueeze(0)</code> 来增加一维(fake batch).</p>
</blockquote>

<p>综上，讲到的模块有，</p>

<ul>
  <li>torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.</li>
  <li>nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.</li>
  <li>nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.</li>
  <li>autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.</li>
</ul>

<p>已经完成的部分，</p>

<ul>
  <li>定义神经网络</li>
  <li>处理输入，反向传播</li>
</ul>

<p>还差，</p>

<ul>
  <li>计算损失</li>
  <li>更新权重(weight)</li>
</ul>

<h2 id="损失函数">损失函数</h2>

<p>损失函数以(output, target) 为输入, 计算一个非负实数来度量输出与目标相差多少。</p>

<p>nn package 有几个不同的损失函数。 nn.MSELoss计算均方误差。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># a dummy target, for example
</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># make it the same shape as output
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-bat highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">Out</span>:

<span class="kd">tensor</span><span class="o">(</span><span class="m">0</span>.7155<span class="o">,</span> <span class="kd">grad_fn</span><span class="o">=&lt;</span><span class="kd">MseLossBackward</span><span class="o">&gt;)</span>
<span class="kd">Now</span><span class="o">,</span> <span class="k">if</span> <span class="kd">you</span> <span class="kd">follow</span> <span class="kd">loss</span> <span class="k">in</span> <span class="kd">the</span> <span class="kd">backward</span> <span class="kd">direction</span><span class="o">,</span> <span class="kd">using</span> <span class="kd">its</span> .grad_fn <span class="kd">attribute</span><span class="o">,</span> <span class="kd">you</span> <span class="kd">will</span> <span class="kd">see</span> <span class="kd">a</span> <span class="kd">graph</span> <span class="kd">of</span> <span class="kd">computations</span> <span class="kd">that</span> <span class="kd">looks</span> <span class="kd">like</span> <span class="kd">this</span>:

<span class="kd">input</span> <span class="o">-&gt;</span> <span class="kd">conv2d</span> <span class="o">-&gt;</span> <span class="kd">relu</span> <span class="o">-&gt;</span> <span class="kd">maxpool2d</span> <span class="o">-&gt;</span> <span class="kd">conv2d</span> <span class="o">-&gt;</span> <span class="kd">relu</span> <span class="o">-&gt;</span> <span class="kd">maxpool2d</span>
      <span class="o">-&gt;</span> <span class="kd">view</span> <span class="o">-&gt;</span> <span class="kd">linear</span> <span class="o">-&gt;</span> <span class="kd">relu</span> <span class="o">-&gt;</span> <span class="kd">linear</span> <span class="o">-&gt;</span> <span class="kd">relu</span> <span class="o">-&gt;</span> <span class="kd">linear</span>
      <span class="o">-&gt;</span> <span class="kd">MSELoss</span>
      <span class="o">-&gt;</span> <span class="kd">loss</span>
</code></pre></div></div>

<p>当调用loss.backward()，整个计算图(graph)从loss开始计算微分。计算图中所有属性requires_grad=True的张量(Tensor)将在.grad属性上累加梯度，.grad也是一个张量。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>  <span class="c1"># MSELoss
</span><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Linear
</span><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># ReLU
</span></code></pre></div></div>

<div class="language-bat highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">Out</span>:

<span class="o">&lt;</span><span class="kd">MseLossBackward</span> <span class="kd">object</span> <span class="nb">at</span> <span class="mh">0x7fcf8ef75e48</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="kd">AddmmBackward</span> <span class="kd">object</span> <span class="nb">at</span> <span class="mh">0x7fcf8ef75f60</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="kd">AccumulateGrad</span> <span class="kd">object</span> <span class="nb">at</span> <span class="mh">0x7fcf8ef75f60</span><span class="o">&gt;</span>
</code></pre></div></div>

<h2 id="backprop">Backprop</h2>

<p>调用loss.backward()开始反向传播，我们需要先清零梯度，否则梯度会累加。</p>

<p>观察反向传播前后conv1.bias的梯度变化。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>     <span class="c1"># zeroes the gradient buffers of all parameters
</span>
<span class="k">print</span><span class="p">(</span><span class="s">'conv1.bias.grad before backward'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'conv1.bias.grad after backward'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-bat highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">Out</span>:

<span class="kd">conv1</span>.bias.grad <span class="kd">before</span> <span class="kd">backward</span>
<span class="kd">tensor</span><span class="o">([</span><span class="m">0</span>.<span class="o">,</span> <span class="m">0</span>.<span class="o">,</span> <span class="m">0</span>.<span class="o">,</span> <span class="m">0</span>.<span class="o">,</span> <span class="m">0</span>.<span class="o">,</span> <span class="m">0</span>.<span class="o">])</span>
<span class="kd">conv1</span>.bias.grad <span class="kd">after</span> <span class="kd">backward</span>
<span class="kd">tensor</span><span class="o">([-</span><span class="m">0</span>.0038<span class="o">,</span>  <span class="m">0</span>.0054<span class="o">,</span>  <span class="m">0</span>.0083<span class="o">,</span>  <span class="m">0</span>.0059<span class="o">,</span>  <span class="m">0</span>.0141<span class="o">,</span> <span class="o">-</span><span class="m">0</span>.0297<span class="o">])</span>
</code></pre></div></div>

<p>Now, we have seen how to use loss functions.</p>

<blockquote>
  <p>*Read Later * 
关于neural network package的<a href="https://pytorch.org/docs/nn">更多细节</a>。</p>
</blockquote>

<h2 id="update-the-weights">Update the weights</h2>

<p>最简单的更新规则是SGD(随机梯度下降，Stochastic Gradient Descent):</p>

<p><code class="language-plaintext highlighter-rouge">weight = weight - learning_rate * gradient</code></p>

<p>用python实现就是：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">f</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div>

<p>有多种更新方式可用，包括SGD, Nesterov-SGD, Adam, RMSProp,等等。我们通过 torch.optim来使用这些方法。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># create your optimizer
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># in your training loop:
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>   <span class="c1"># zero the gradient buffers
</span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>    <span class="c1"># Does the update
</span></code></pre></div></div>

<blockquote>
  <p><em>NOTE</em><br />
使用 optimizer.zero_grad()将梯度手动清零。这是因为在反向传播中，计算梯度时会累加。
比如:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>假如
a := f(x)
b := g(x)
y := a+b
则y.grad := a.grad + b.grad
</code></pre></div>  </div>
</blockquote>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Rymoon/rymoon.github.io">rymoon.github.io</a> is maintained by <a href="https://github.com/Rymoon">Rymoon</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
  </body>
</html>
