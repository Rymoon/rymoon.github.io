<!DOCTYPE html>
<html lang="en-US">
  <head>
     <meta charset="utf-8">
     
         <script type="text/x-mathjax-config">
 MathJax.Hub.Config({
     TeX: {
         equationNumbers: {
             autoNumber: "AMS"
         }
     },
     tex2jax: {
         inlineMath: [ ['$','$'] ],
         processEscapes: true,
     }
 });
 </script>
 <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
 </script>

     


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>1 gradient descent | Rymoon’s Website</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="1 gradient descent" />
<meta name="author" content="stg1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction to Machine Learning Algorithms and Realizations 1 By Jiaheng Cui In this chapter, we’ll focus on an optimization method - gradient descent, and we’ll explain how gradient descent works in fine-tuning models. Finally we’ll introduce some revised method of gradient descent." />
<meta property="og:description" content="Introduction to Machine Learning Algorithms and Realizations 1 By Jiaheng Cui In this chapter, we’ll focus on an optimization method - gradient descent, and we’ll explain how gradient descent works in fine-tuning models. Finally we’ll introduce some revised method of gradient descent." />
<link rel="canonical" href="http://localhost:4000/2020/11/12/1-Gradient-Descent.html" />
<meta property="og:url" content="http://localhost:4000/2020/11/12/1-Gradient-Descent.html" />
<meta property="og:site_name" content="Rymoon’s Website" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-12T00:00:00+08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/2020/11/12/1-Gradient-Descent.html","headline":"1 gradient descent","dateModified":"2020-11-12T00:00:00+08:00","datePublished":"2020-11-12T00:00:00+08:00","author":{"@type":"Person","name":"stg1"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/11/12/1-Gradient-Descent.html"},"description":"Introduction to Machine Learning Algorithms and Realizations 1 By Jiaheng Cui In this chapter, we’ll focus on an optimization method - gradient descent, and we’ll explain how gradient descent works in fine-tuning models. Finally we’ll introduce some revised method of gradient descent.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=273ec00d6f02ee547cdd583a95466fd9c76bed0d">
  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">1 gradient descent</h1>
      <h2 class="project-tagline">A humble website.</h2>
      
        <a href="https://github.com/Rymoon/rymoon.github.io" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1>1 gradient descent</h1>

<p>
  12 Nov 2020
  
  
    - <a href="/authors/stg1.html">吕之豪 崔嘉珩 肖韵竹 戴文娇</a>
  
</p>

<h1 id="introduction-to-machine-learning-algorithms-and-realizations-1"><em>Introduction to Machine Learning</em> Algorithms and Realizations 1</h1>

<h2 id="by-jiaheng-cui">By Jiaheng Cui</h2>
<blockquote>
  <p>In this chapter, we’ll focus on an optimization method - gradient descent, and we’ll explain how gradient descent works in fine-tuning models. Finally we’ll introduce some revised method of gradient descent.</p>
</blockquote>

<!-- more -->

<h2 id="1gradient-descent">1.Gradient Descent</h2>
<h3 id="1-basic-thoughts">(1) Basic thoughts</h3>
<p>Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.</p>

<p>We want to solve the following optimization problem:
\(\mathop{\min}\limits_{x \in R^n} f(x)\)</p>

<p>Let $f(x)$ be differentiable, and $ x^{*} $ be the corresponding local minimum of $f(x)$.</p>

<p>We’ll choose a starting point $x_0$ and use the following equation to implement the iteration:
\(x_{k+1} = x_k + step_k p_k, k = 0,1,2,...\)</p>

<p>Where $step_k$ is the k-th step size (or learning rate), and $p_k$ is the iteration direction.</p>

<p>In Gradient Descent, as $f(x)$ <strong>decreases the fastest</strong> if one goes from $x_k$ in the direction of the <strong>negative</strong> gradient of $f$ at $x_k$, which is $-\nabla f(x_k)$, we’ll let $p_k = -\nabla f(x_k)$ at the k-th iteration.</p>

<p>Then we can use line search to solve $step_k$, that is, let $step_k = \mathop{\arg\min}\limits_{step_k \ge 0} f(x_k + step_k p_k)$. Or we can use grid search to find a fixed good step size.</p>

<p>Note that if $f(x)$ is convex, then we have only one minimum point $x^{<em>}$, so the minimum we derived from gradient descent is $x^{</em>}$ itself (assuming there were no error).</p>

<h3 id="2-quadratic-situation">(2) Quadratic situation</h3>
<p>If $f(x)$ is positive definite and quadratic, that is \(f(x) = \frac{1}{2}x^T A x + b^T x + c, A \gt 0\)</p>

<p>Where $A \gt 0$ means $A$ is positive definite, and $x$ here is a vector. Then $f(x)$ is a convex function on $R^{n \times 1}$.</p>

<p>Let $g_k = \nabla f(x_k) = Ax_k + b$, then the iteration direction $p_k = -g_k$, now $x_{k+1} = x_k - step_k g_k$.</p>

<p>Then we can derive an explicit formula of $step_k$. $step_k$ would make $f(x)$ to its minimum at direction $g_k$, so $step_k$ should be a minimizer of $\phi(\alpha) = f(x_k - \alpha g_k)$.</p>

<p>Since $\phi’(\alpha) = ((x_k - \alpha g_k)^T A + b^T) (-g_k) $ and $\phi’(step_k) = 0$, $step_k = \frac{g_k^T g_k}{g_k^T A g_k}$.</p>

<p>As a result, \(x_{k+1} = x_k - \frac{g_k^T g_k}{g_k^T A g_k} g_k, g_k = Ax_k + b\)</p>

<h3 id="3-algorithm">(3) Algorithm</h3>
<p>Input: target function $f(x)$, gradient function $g(x) = \nabla f(x)$, tolerance $\epsilon$, starting point $x_0 \in R^{n \times 1}$, max iteration number $k_{max}$.</p>

<p>Output: a minimum point of $f(x)$, $x^{*}$.</p>

<p>Step1: load $x_0$, and set $k = 0$.</p>

<p>Step2: calculate $f(x_k)$</p>

<p>Step3: calculate $g_k = g(x_k)$, if $\parallel g_k\parallel \lt \epsilon$, stop iteration and let $x^* = x_k$; otherwise, let $p_k = -g_k$ and solve $step_k = \mathop{\arg\min}\limits_{step_k \ge 0} f(x_k + step_k p_k)$.</p>

<p><strong>Note: in this chapter, $\parallel \parallel$ means $L^2$ norm.</strong></p>

<p>Step4: let $x_{k+1} = x_k + step_k p_k$, calculate $f(x_{k+1})$.</p>

<p>Step5: if $\parallel f(x_{k+1}) - f(x_{k})\parallel \lt \epsilon$ or $\parallel x_{k+1} - x_{k}\parallel \lt \epsilon$, stop iteration and let $x^* = x_{k+1}$; otherwise, let $k = k+1$.</p>

<p>Step6: if $k = k_{max}$, the algorithm doesn’t converge, print “Does not converge, calculation failed.”; otherwise, return to Step3.</p>

<h3 id="4-code">(4) Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<p>Generally, solving $step_k$ would be inconvenient. We’ll just show two situations: fixed learning rate and quadratic situation.</p>

<p>&lt;1&gt; Fixed learning rate $\eta$ given by user, then $step_k = \eta, k=0,1,2,…$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent_fixed_eta</span><span class="p">(</span><span class="n">f_x</span><span class="p">,</span> <span class="n">g_x</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">k_max</span><span class="p">):</span>
    <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span><span class="c1">#range(n) would produce a sequence: 0, 1, ..., n-1
</span>        <span class="n">g_k</span> <span class="o">=</span> <span class="n">g_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span><span class="c1">#np.linalg.norm(g_k) is the L2 norm of g_k
</span>            <span class="k">return</span> <span class="p">(</span><span class="n">x_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p_k</span> <span class="o">=</span> <span class="o">-</span> <span class="n">g_k</span>
            <span class="n">x_k_new</span> <span class="o">=</span> <span class="n">x_k</span> <span class="o">+</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">p_k</span>
            
            <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x_k_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_k_new</span> <span class="o">-</span> <span class="n">x_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">x_k_new</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
                <span class="k">break</span>
                
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_k_new</span>
                <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="n">k_max</span><span class="p">):</span>
                    <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<p>&lt;2&gt; Quadratic situation, $A$ should be given by user:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent_quadratic</span><span class="p">(</span><span class="n">f_x</span><span class="p">,</span> <span class="n">g_x</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">k_max</span><span class="p">):</span>
    <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">g_k</span> <span class="o">=</span> <span class="n">g_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
            
        <span class="k">else</span><span class="p">:</span>
            <span class="n">step_k</span> <span class="o">=</span> <span class="n">g_k</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span><span class="o">/</span><span class="n">g_k</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span><span class="c1">#.T is the action of matrix transpose, .dot() is the action of matrix multiplication
</span>            <span class="n">x_k_new</span> <span class="o">=</span> <span class="n">x_k</span> <span class="o">-</span> <span class="n">step_k</span> <span class="o">*</span> <span class="n">g_k</span>
            
            <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x_k_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_k_new</span> <span class="o">-</span> <span class="n">x_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">x_k_new</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
                <span class="k">break</span>
                
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_k_new</span>
                <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="n">k_max</span><span class="p">):</span>
                    <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<p>Let’s do a quick test. Let $f(x_1, x_2) = x_1^2 + 4x_2^2$. Obviously $f(x)$ is convex, and the only minimum is $(0,0)$.</p>

<p>We can convert $f(x_1, x_2)$ into its matrix form: $f(x) = \frac{1}{2}x^T Ax, where x = (x_1,x_2)^T, A = \left[\begin{matrix} 2 &amp; 0\ 0 &amp; 8\ \end{matrix}\right] $. Then $g(x) = \nabla f(x) = Ax$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">f_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">g_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">gradient_descent_fixed_eta</span><span class="p">(</span><span class="n">f_test</span><span class="p">,</span> <span class="n">g_test</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">gradient_descent_quadratic</span><span class="p">(</span><span class="n">f_test</span><span class="p">,</span> <span class="n">g_test</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([3.77789319e-03, 3.35544320e-18]), 24)
(array([ 1.00365696e-03, -6.27285599e-05]), 6)
</code></pre></div></div>

<p>As we can see, if $f(x)$ is a quadratic function, it’s better to use the explicit $step_k$. The iteration number using $step_k$ is 6 while using fixed learning rate is 24.</p>

<h3 id="5-what-can-we-do-with-gradient-descent-in-machine-learning">(5) What can we do with Gradient Descent in Machine Learning?</h3>

<p>In machine learning tasks, we usually have a training set $S = \left{(x_1,y_1),(x_2,y_2),…,(x_n,y_n)\right}, where x_i \in R^{p \times 1} ,y_i \in R$, a model with parameter vector $\theta = (\theta_1, \theta_2, … ,\theta_p)^T \in R^{p \times 1}$. And we want to minimize the loss function $L(\theta) = \frac{1}{n}\sum_{i=1}^{n}L(f(x_i,\theta),y_i)$.</p>

<p>Suppose we want to train a linear model and choose MSE as our loss function:
\(MSE(\theta) = \frac{1}{n} \parallel X \theta - y\parallel^2= \frac{1}{n}\sum_{i=1}^{n}(\theta^T x_i - y_i)^2\)</p>

<p>Where$X = (x_1, x_2, …, x_n)^T \in R^{n \times p}, y = (y_1, y_2, …, y_n)^T \in R^{n \times 1}$.</p>

<p>If we want to minimize MSE by gradient descent, we need to compute its gradient:
\(\nabla MSE(\theta) = \frac{2}{n} X^T(X\theta-y)\)</p>

<p>The iteration formula is $\theta_{k+1} = \theta_k - step_k \nabla MSE(\theta)$.</p>

<p>If we put MSE and its gradient into python code, then we can use gradient descent to find the best $\theta$ of our model. We’ll use a fixed step size $\eta$ given by the user:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MSE</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">MSE_gradient</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">/</span><span class="n">n</span> <span class="o">*</span> <span class="n">X</span><span class="o">^</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">MSE_gradient_descent_fixed_eta</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">k_max</span><span class="p">):</span>
    <span class="n">theta_k</span> <span class="o">=</span> <span class="n">theta_0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">g_k</span> <span class="o">=</span> <span class="n">MSE_gradient</span><span class="p">(</span><span class="n">theta_k</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">theta_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">theta_k_new</span> <span class="o">=</span> <span class="n">theta_k</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g_k</span>
            
            <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">MSE</span><span class="p">(</span><span class="n">theta_k_new</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">MSE</span><span class="p">(</span><span class="n">theta_k</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta_k_new</span> <span class="o">-</span> <span class="n">theta_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">theta_k_new</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
                <span class="k">break</span>
                
            <span class="k">else</span><span class="p">:</span>
                <span class="n">theta_k</span> <span class="o">=</span> <span class="n">theta_k_new</span>
                <span class="k">if</span><span class="p">(</span><span class="n">k</span><span class="o">==</span><span class="n">k_max</span><span class="p">):</span>
                    <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<h2 id="2conjugate-gradient-descent">2.Conjugate Gradient Descent</h2>
<h3 id="1-quadratic-situation">(1) Quadratic situation</h3>
<p>When $x_k$ is near $x^*$, especially when $f(x)$ is complicated, convergence would be slower and slower. Conjugate gradient descent can perform better when this problem occurs.</p>

<p>Let $A$ be a positive definite matrix, $Q_1$ and $Q_2$ are two non-zero vector. Then $Q_1$ and $Q_2$ are conjugate with respect to $A$ if $Q_1^T A Q_2 = 0$.</p>

<p>We’ll still consider the quadratic problem:\(\mathop{\min}\limits_{x \in R^n} f(x) = \mathop{\min}\limits_{x \in R^n}\frac{1}{2}x^T A x + b^T x + c, A \gt 0\)</p>

<p>The iteration formula here is $x_{k+1} = x_k + \alpha_k p_k$.</p>

<p>Previously, $p_{k+1} = -\nabla f(x_{k+1})$. Now we can use $step_k$ and $p_k$ to revise $p_{k+1}$:
\(p_{k+1} =  -\nabla f(x_{k+1}) + step_k p_k, p_0 = -\nabla f(x_0)\)</p>

<p>Let $p_{k+1}$ and $p_k$ be conjugate, then $0 = p_{k+1}^T A p_k = -\nabla f(x_{k+1})A p_k + step_k p_k^TA p_k$. Then we can get $step_k = \frac{-\nabla f(x_{k+1})^T A p_k}{p_k^TA p_k}$.</p>

<p>Then by line search can we get $\alpha_k = \frac{p_k^T(-b-Ax_k)}{p_k^T A p_k}$.</p>

<p>As a result, in every iteration we need to first update $p_k = - \nabla f(x_k) - \frac{\nabla f(x_{k})^T A p_{k-1}}{p_{k-1}^TA p_{k-1}}p_{k-1}$. Then use the new $p_k$ to calculate $\alpha_k = \frac{p_k^T(-b-Ax_k)}{p_k^T A p_k}$. Finally get $x_{k+1} = x_k + \alpha_k p_k$.</p>

<h3 id="2-algorithm">(2) Algorithm</h3>
<p>Input: target function $f(x)$, gradient function $g(x) = \nabla f(x)$, tolerance $\epsilon$, starting point $x_0 \in R^{n \times 1}$, positive definite matrix $A \in R^{n \times n}$ used in $f(x)$, vector $b \in R^{n \times 1}$ used in $f(x)$, max iteration number $k_{max}$.</p>

<p>Output: a minimum point of $f(x)$, $x^{*}$.</p>

<p>Step1: load $x_0$, and set $k = 0$.</p>

<p>Step2: calculate $f(x_k)$.</p>

<p>Step3: calculate $g_k = g(x_k)$, if $\parallel g_k\parallel \lt \epsilon$, stop iteration and let $x^* = x_k$; otherwise, if $k = 0$, let $p_k = -g_k$, else if $k &gt; 0$, let $p_k = -g_k  - \frac{g_k^T A p_{k-1}}{p_{k-1}^TA p_{k-1}}p_{k-1}$ and solve $\alpha_k = \frac{p_k^T(-b-Ax_k)}{p_k^T A p_k}$.</p>

<p>Step4: let $x_{k+1} = x_k + \alpha_k p_k$, calculate $f(x_{k+1})$.</p>

<p>Step5: if $\parallel f(x_{k+1}) - f(x_{k})\parallel \lt \epsilon$ or $\parallel x_{k+1} - x_{k}\parallel \lt \epsilon$, stop iteration and let $x^* = x_{k+1}$; otherwise, let $k = k+1$.</p>

<p>Step6: if $k = k_{max}$, the algorithm doesn’t converge, print “Does not converge, calculation failed.”;otherwise, return to Step3.</p>

<h3 id="3-code">(3) Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">conjugate_gradient_descent_quadratic</span><span class="p">(</span><span class="n">f_x</span><span class="p">,</span> <span class="n">g_x</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">k_max</span><span class="p">):</span>
    <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_k_old</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">x_0</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">g_k</span> <span class="o">=</span> <span class="n">g_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
            
        <span class="k">elif</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">p_k</span> <span class="o">=</span> <span class="o">-</span><span class="n">g_k</span>
            
        <span class="k">elif</span><span class="p">(</span><span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">p_k</span> <span class="o">=</span> <span class="o">-</span><span class="n">g_k</span> <span class="o">-</span> <span class="n">g_k</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">p_k_old</span><span class="p">)</span><span class="o">/</span><span class="n">p_k_old</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">p_k_old</span><span class="p">)</span> <span class="o">*</span> <span class="n">p_k_old</span>
            
        <span class="n">alpha_k</span> <span class="o">=</span> <span class="n">p_k</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">-</span><span class="n">A</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_k</span><span class="p">))</span><span class="o">/</span><span class="n">p_k</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">p_k</span><span class="p">)</span>
        <span class="n">x_k_new</span> <span class="o">=</span> <span class="n">x_k</span> <span class="o">+</span> <span class="n">alpha_k</span> <span class="o">*</span> <span class="n">p_k</span>
            
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x_k_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_k_new</span> <span class="o">-</span> <span class="n">x_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x_k_new</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
                
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_k_new</span>
            <span class="n">p_k_old</span> <span class="o">=</span> <span class="n">p_k</span>
            <span class="k">if</span><span class="p">(</span><span class="n">k</span><span class="o">==</span><span class="n">k_max</span><span class="p">):</span>
                <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<p>Let’s do a quick test. Let $f(x_1, x_2) = x_1^2 - 2x_1 + 4x_2^2 - 16x_2$. Obviously $f(x)$ is convex, and the only minimum is $(1,2)$.</p>

<p>We can convert $f(x_1, x_2)$ into its matrix form: $f(x) = \frac{1}{2}x^T Ax + b^Tx, where x = (x_1,x_2)^T, A = \left[\begin{matrix} 2 &amp; 0\ 0 &amp; 8\ \end{matrix}\right], b = (-2, -16)^T $. Then $g(x) = \nabla f(x) = Ax + b$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">16</span><span class="p">])</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">f_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">g_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">print</span><span class="p">(</span><span class="n">conjugate_gradient_descent_quadratic</span><span class="p">(</span><span class="n">f_test</span><span class="p">,</span> <span class="n">g_test</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([0.99859738, 1.99989539]), 10)
</code></pre></div></div>

<h2 id="3stochastic-gradient-descent">3.Stochastic Gradient Descent</h2>
<h3 id="1-basic-thoughts-1">(1) Basic thoughts</h3>
<p>In part 1, we can see when computing loss function, we need to use $x_1, x_2, …, x_n$, i.e. all samples. Hence when $n$ is large, it’ll be very slow to compute loss function and its gradient.</p>

<p>We now introduce Stochastic Gradient Descent, which only use one random sample to compute loss function and its gradient. This would make the algorithm much faster when dealing with huge training sets, and possibly to jump out of the neighborhood of a local minimum and find the global minimum. However, when it end up close to the minimum, it’ll bounce around, thus never find the optimal parameter but only a relatively good parameter.</p>

<p>We’ll deal with the MSE situation. Every iteration we’ll randomly pick out one $x_i$ from $X$, and corresponding $y_i$ from $y$, then input them into the following formula:</p>

\[MSE(\theta | x_i, y_i) = (\theta^T x_i - y_i)^2\]

<table>
  <tbody>
    <tr>
      <td>Then we can compute the gradient: $\nabla MSE(\theta</td>
      <td>x_i, y_i) = 2(x_i^T\theta-y_i) x_i$.</td>
    </tr>
  </tbody>
</table>

<p>The iteration formula is $\theta_{k+1} = \theta_k - step_k \nabla MSE(\theta | x_i, y_i)$. Here we’ll use a simple learning schedule: $step_k = \frac{5}{n+k+50}$, this would make step size smaller and smaller and restrain $\theta$ from bouncing around.</p>
<h3 id="2-code">(2) Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MSE_one_sample</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span><span class="o">-</span><span class="n">y_i</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">MSE_gradient_one_sample</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_i</span><span class="o">^</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y_i</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_i</span>

<span class="k">def</span> <span class="nf">learning_schedule</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">5</span><span class="o">/</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">MSE_stochastic_gradient_descent</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">k_max</span><span class="p">):</span>
    <span class="n">theta_k</span> <span class="o">=</span> <span class="n">theta_0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="c1">#randint(1, n+1) will produce a random integer from {1,2,...,n}
</span>        <span class="n">g_k</span> <span class="o">=</span> <span class="n">MSE_gradient_one_sample</span><span class="p">(</span><span class="n">theta_k</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">theta_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eta</span> <span class="o">=</span> <span class="n">learning_schedule</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">theta_k_new</span> <span class="o">=</span> <span class="n">theta_k</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g_k</span>
            
            <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">MSE_one_sample</span><span class="p">(</span><span class="n">theta_k_new</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">MSE_one_sample</span><span class="p">(</span><span class="n">theta_k</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta_k_new</span> <span class="o">-</span> <span class="n">theta_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">theta_k_new</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
                <span class="k">break</span>
                
            <span class="k">else</span><span class="p">:</span>
                <span class="n">theta_k</span> <span class="o">=</span> <span class="n">theta_k_new</span>
                <span class="k">if</span><span class="p">(</span><span class="n">k</span><span class="o">==</span><span class="n">k_max</span><span class="p">):</span>
                    <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<h2 id="4mini-batch-stochastic-gradient-descent">4.Mini-batch Stochastic Gradient Descent</h2>
<h3 id="1-basic-thoughts-2">(1) Basic thoughts</h3>
<p>If we use small batches of samples every time to calculate loss function rather than one sample or all samples, we could somehow combine the advantages of gradient descent and stochastic gradient descent.</p>

<p>We’ll still deal with the MSE situation. Every iteration we’ll randomly pick out $m (1\le m \le n)$ random $x_i$s (denoted as $x_{i_1},…,x_{i_m}$) from $X$, and corresponding $y_i$s (denoted as $y_{i_1},…,y_{i_m}$) from $y$. By convention we iterate by rounds of $m$ iterations, each round is called an epoch, we’ll do $n_epoch$ epochs.</p>

<p>Let $X_b = (x_{i_1},…,x_{i_m})^T \in R^{m \times p}, y_b = (y_{i_1},…,y_{i_m})^T \in R^{m \times 1}$, then input them into the following formula:</p>

\[MSE(\theta | X_b, y_b) = \frac{1}{m} \parallel X_b \theta - y_b\parallel^2= \frac{1}{m}\sum_{k=1}^{m}(\theta^T x_{i_k} - y_{i_k})^2\]

<table>
  <tbody>
    <tr>
      <td>Then we can compute the gradient: $$\nabla MSE(\theta</td>
      <td>X_b, y_b) = \frac{2}{m} X_b^T(X_b\theta-y_b)$$.</td>
    </tr>
  </tbody>
</table>

<p>The iteration formula is $\theta_{k+1} = \theta_k - step_k \nabla MSE(\theta | X_b, y_b)$. Note that we’ll plug the current epoch number into the learning schedule: $step_k = \frac{5}{epoch*n+k+50}$.</p>
<h3 id="2-code-1">(2) Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">draw_m_samples_from_n</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="k">if</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">m</span> <span class="o">&gt;</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">return</span><span class="p">(</span><span class="s">"Invalid batch size!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)])</span><span class="c1"># This would produce m random integer from{1,2,...,n}
</span>        <span class="k">return</span> <span class="n">L</span>
    
<span class="c1"># Note that we'll use the same MSE and gradient as part 1.
# def MSE(theta, X, y):
#     n = X.shape[0]
#     return 1/n * np.linalg.norm(X.dot(theta)-y)^2
</span>
<span class="c1"># def MSE_gradient(theta, X, y):
#     n = X.shape[0]
#     return 2/n * X^T.dot(X.dot(theta)-y)
</span>
<span class="c1"># We'll use the same learning schedule as part 3.
# def learning_schedule(t):
#    return 5/(t + 50)
</span>
<span class="k">def</span> <span class="nf">MSE_mini_batch_stochastic_gradient_descent</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">k_max</span><span class="p">,</span> <span class="n">n_epoch</span><span class="p">):</span>
    <span class="n">theta_k</span> <span class="o">=</span> <span class="n">theta_0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_epoch</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">L</span> <span class="o">=</span> <span class="n">draw_m_samples_from_n</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
            <span class="n">g_k</span> <span class="o">=</span> <span class="n">MSE_gradient</span><span class="p">(</span><span class="n">theta_k</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">L</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">L</span><span class="p">])</span>
        
            <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">theta_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
                <span class="k">break</span>
        
            <span class="k">else</span><span class="p">:</span>
                <span class="n">eta</span> <span class="o">=</span> <span class="n">learning_schedule</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>
                <span class="n">theta_k_new</span> <span class="o">=</span> <span class="n">theta_k</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g_k</span>
            
                <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">MSE</span><span class="p">(</span><span class="n">theta_k_new</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">MSE</span><span class="p">(</span><span class="n">theta_k</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta_k_new</span> <span class="o">-</span> <span class="n">theta_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                    <span class="k">return</span> <span class="p">(</span><span class="n">theta_k_new</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
                    <span class="k">break</span>
                
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">theta_k</span> <span class="o">=</span> <span class="n">theta_k_new</span>
                    <span class="k">if</span><span class="p">(</span><span class="n">k</span><span class="o">==</span><span class="n">k_max</span><span class="p">):</span>
                        <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<h2 id="references">References:</h2>

<p>1.统计学习方法（第2版）- 李航</p>

<p>2.机器学习 - 周志华</p>

<p>3.Hands-on Machine Learning with Scikit-Learn, Keras &amp; TensorFlow, 2nd Edition - Aurélien Géron</p>

<p>4.Conjugate gradient method - Wikipedia</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Rymoon/rymoon.github.io">rymoon.github.io</a> is maintained by <a href="https://github.com/Rymoon">Rymoon</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
  </body>
</html>
