<!DOCTYPE html>
<html lang="en-US">
  <head>
     <meta charset="utf-8">
     
         <script type="text/x-mathjax-config">
 MathJax.Hub.Config({
     TeX: {
         equationNumbers: {
             autoNumber: "AMS"
         }
     },
     tex2jax: {
         inlineMath: [ ['$','$'] ],
         processEscapes: true,
     }
 });
 </script>
 <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
 </script>

     


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>2 newtons method and quasi Newton method | Rymoon’s Website</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="2 newtons method and quasi Newton method" />
<meta name="author" content="stg1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction to Machine Learning Algorithms and Realizations 2 By Jiaheng Cui In this chapter, we’ll focus on Newton’s Method and DFP Quasi-Newton Method. If you are interested, you can also see how Zhihao Lyu used Newton’s Method to solve the MLE of a Gamma distribution" />
<meta property="og:description" content="Introduction to Machine Learning Algorithms and Realizations 2 By Jiaheng Cui In this chapter, we’ll focus on Newton’s Method and DFP Quasi-Newton Method. If you are interested, you can also see how Zhihao Lyu used Newton’s Method to solve the MLE of a Gamma distribution" />
<link rel="canonical" href="http://localhost:4000/2020/11/12/2-Newtons-Method-and-Quasi-Newton-Method.html" />
<meta property="og:url" content="http://localhost:4000/2020/11/12/2-Newtons-Method-and-Quasi-Newton-Method.html" />
<meta property="og:site_name" content="Rymoon’s Website" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-12T00:00:00+08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/2020/11/12/2-Newtons-Method-and-Quasi-Newton-Method.html","headline":"2 newtons method and quasi Newton method","dateModified":"2020-11-12T00:00:00+08:00","datePublished":"2020-11-12T00:00:00+08:00","author":{"@type":"Person","name":"stg1"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/11/12/2-Newtons-Method-and-Quasi-Newton-Method.html"},"description":"Introduction to Machine Learning Algorithms and Realizations 2 By Jiaheng Cui In this chapter, we’ll focus on Newton’s Method and DFP Quasi-Newton Method. If you are interested, you can also see how Zhihao Lyu used Newton’s Method to solve the MLE of a Gamma distribution","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=273ec00d6f02ee547cdd583a95466fd9c76bed0d">
  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">2 newtons method and quasi Newton method</h1>
      <h2 class="project-tagline">A humble website.</h2>
      
        <a href="https://github.com/Rymoon/rymoon.github.io" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1>2 newtons method and quasi Newton method</h1>

<p>
  12 Nov 2020
  
  
    - <a href="/authors/stg1.html">吕之豪 崔嘉珩 肖韵竹 戴文娇</a>
  
</p>

<h1 id="introduction-to-machine-learning-algorithms-and-realizations-2"><em>Introduction to Machine Learning</em> Algorithms and Realizations 2</h1>

<h2 id="by-jiaheng-cui">By Jiaheng Cui</h2>
<blockquote>
  <p>In this chapter, we’ll focus on Newton’s Method and DFP Quasi-Newton Method. If you are interested, you can also see how Zhihao Lyu used Newton’s Method to solve the MLE of a Gamma distribution</p>
</blockquote>

<!-- more -->

<h2 id="1newtons-method">1.Newton’s Method</h2>
<h3 id="1-basic-thoughts">(1) Basic thoughts</h3>
<p>We introduced that gradient descent is a first-order optimization algorithm for finding a local minimum of a differentiable function in the last chapter. Now we introduce a second-order method, Newton’s Method, or the Newton-Raphson Method.</p>

<p>We want to solve the following optimization problem:
\(\mathop{\min}\limits_{x \in R^{n \times 1}} f(x)\)</p>

<p>Let $f(x)$ be <strong>twice</strong> differentiable, and $ x^{*} $ be the corresponding local minimum of $f(x)$.</p>

<p>The 2nd-order Taylor expansion of $f(x)$ is $f(x+\Delta x) = f(x) + g(x)\Delta x + \frac{1}{2} \Delta x^T H(x)\Delta x+ O(\parallel \Delta x \parallel^3)$, where $g(x) = \nabla f(x)$ is the gradient of $f$ at point $x$, $H(x) = \nabla^2 f(x)$ is the Hessian matrix of $f$ at point $x$. If we ignore the $O(\parallel \Delta x \parallel^3)$ term, then $f(x+\Delta x) \approx f(x) + \nabla f(x)\Delta x + \frac{1}{2} \Delta x^T H(x)\Delta x$, we can use this formula to derive an iterative method to solve $x^*$.</p>

<p>We’ll choose a starting point $x_0$ and use the following equation to implement the iteration:
$x_{k+1} = x_k - H(x)^{-1} g_k = x_k - \left[ \nabla^2 f(x) \right]^{-1} \nabla f(x)$, $k = 0,1,2,…$</p>

<p>Again, if $f(x)$ is convex, then we have only one minimum point $x^{<em>}$, so the minimum we derived from gradient descent is $x^{</em>}$ itself (assuming there were no error).</p>

<h3 id="2-algorithm">(2) Algorithm</h3>
<p>Input: target function $f(x)$, gradient function $g(x) = \nabla f(x)$, Hessian matrix $H(x) = \nabla^2 f(x)$, tolerance $\epsilon$, starting point $x_0 \in R^{n \times 1}$, max iteration number $k_{max}$.</p>

<p>Output: a minimum point of $f(x)$, $x^{*}$.</p>

<p>Step1: load $x_0$, and set $k = 0$.</p>

<p>Step2: calculate $f(x_k)$</p>

<p>Step3: calculate $g_k = g(x_k)$, if $\parallel g_k\parallel \lt \epsilon$, stop iteration and let $x^* = x_k$; otherwise, let $p_k = -g_k$.</p>

<p>Step4: calculate $H_k = H(x_k)$, if $H_k$ is non-invertible, print “Hessian matrix non-invertible, calculation failed.”, stop iteration; otherwise, calculate $H_{k}^{-1} = H^{-1}(x_k)$.</p>

<p>Step4: let $x_{k+1} = x_k + H_k^{-1} p_k$, calculate $f(x_{k+1})$.</p>

<p>Step5: if $\parallel f(x_{k+1}) - f(x_{k})\parallel \lt \epsilon$ or $\parallel x_{k+1} - x_{k}\parallel \lt \epsilon$, stop iteration and let $x^* = x_{k+1}$; otherwise, let $k = k+1$.</p>

<p>Step6: if $k = k_{max}$, the algorithm doesn’t converge, print “Does not converge, calculation failed.”; otherwise, return to Step3.</p>

<h3 id="3-code">(3) Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Newton</span><span class="p">(</span><span class="n">f_x</span><span class="p">,</span> <span class="n">g_x</span><span class="p">,</span> <span class="n">H_x</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">k_max</span> <span class="o">=</span> <span class="mf">1e5</span><span class="p">):</span>
    <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">g_k</span> <span class="o">=</span> <span class="n">g_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p_k</span> <span class="o">=</span> <span class="o">-</span> <span class="n">g_k</span>
            <span class="n">H_k</span> <span class="o">=</span> <span class="n">H_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span>
            <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">det</span><span class="p">(</span><span class="n">H_k</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">return</span> <span class="s">"Hessian matrix non-invertible, calculation failed."</span>
            
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x_k_new</span> <span class="o">=</span> <span class="n">x_k</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">H_k</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">p_k</span><span class="p">)</span>
                <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x_k_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_k_new</span> <span class="o">-</span> <span class="n">x_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                    <span class="k">return</span> <span class="p">(</span><span class="n">x_k_new</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
                    <span class="k">break</span>
                
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_k_new</span>
                    <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="n">k_max</span><span class="p">):</span>
                        <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<p>Let $f(X) = (x_1 − x_2)^3 + (x_1 + 3x_2)^2$.</p>

<p>Then $g(x) = \nabla f(x) = (3(x_1 - x_2)^2 + 2(x_1 + 3x_2), -3(x_1 - x_2)^2 + 6(x_1 + 3x_2))^T$.</p>

<p>And $H(x) = \nabla^2 f(x) = \left[\begin{matrix} 6(x_1 - x_2) + 2 &amp; -6(x_1 - x_2) + 6\ -6(x_1 - x_2) + 6 &amp; 6(x_1 - x_2) + 18\ \end{matrix}\right]$.</p>

<p>Let the starting point be $x_0 = (1,2)^T$, we’ll use Newton’s method to find a minimum point of $f(x)$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">f_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">g_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x_2</span><span class="p">),</span> <span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">6</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x_2</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">H_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">6</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">18</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">Newton</span><span class="p">(</span><span class="n">f_test</span><span class="p">,</span> <span class="n">g_test</span><span class="p">,</span> <span class="n">H_test</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([-0.00585938,  0.00195313]), 6)
</code></pre></div></div>

<h3 id="4-use-newtons-method-to-solve-the-mle-of-gamma-distribution">(4) Use Newton’s Method to solve the MLE of gamma distribution</h3>

<p>Please refer to the passage written by Zhihao Lyu:</p>

<p>https://github.com/jimcui3/Introduction-to-Machine-Learning/blob/main/2020-10-26-MLE%20with%20Newton%20Raphson.md</p>

<h2 id="2quasi-newton-method">2.Quasi-Newton Method</h2>
<h3 id="1-basic-thoughts-1">(1) Basic thoughts</h3>
<p>Note that during the iteration, $H(x_k)$ is not always invertible. If there exists a $k$ s.t. $det(H(x_k)) = 0$, we cannot perform the Newton’s method iteration. Thus there are many thoughts to use a series of invertible matrices to estimate <strong>the inverse of the Hessian matrix</strong>, we still denote these estimators as $\left{ H_k \right}$. These methods are called Quasi-Newton Methods. Everytime we’ll use $H_k$ compute $x_{k+1}$, , then if didn’t reach convergence, we’ll get $H_{k+1}$ by a certain formula concerning $x_k$, $g_k$ and $H_k$. Then use $x_{k+1}$, $g_{k+1}$ and $H_{k+1}$ to compute $x_{k+2}$.</p>

<p>The most common Quasi-Newton Methods include DFP Quasi-Newton Method and BFGS Quasi-Newton Method. We’ll only talk about DFP Quasi-Newton Method, those who are interested in BFGS Quasi-Newton Method can look it up here:</p>

<p>https://en.wikipedia.org/wiki/BFGS_method</p>

<h3 id="2-dfp-quasi-newton-method">(2) DFP Quasi-Newton Method</h3>
<p>Let $H_0 = I_n$, where $I_n$ is the identity matrix of order $n$.</p>

<p>Let $\Delta x_k = x_{k+1} - x_k$, $\Delta g_k = g_{k+1} - g_k$. Note that we do this step after we got $x_{k+1}$ (and thus $g_{k+1}$) from the iteration formula and only when we won’t stop at $x_{k+1}$.</p>

<p>The iterative formula of $H_k$ is listed as follows:
\(H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^T}{\Delta x_k^T \Delta g_k} - \frac{H_k \Delta g_k \Delta g_k^T H_k}{\Delta g_k^T H_k \Delta g_k}\)</p>

<p>Note: It may be a little difficult to understand the double iteration loops and their sequential orders, if so, please review the following algorithm carefully and make sure you understand it clearly!</p>

<h3 id="3-algorithm">(3) Algorithm</h3>
<p>Input: target function $f(x)$, gradient function $g(x) = \nabla f(x)$, tolerance $\epsilon$, starting point $x_0 \in R^{n \times 1}$, max iteration number $k_{max}$.</p>

<p>Output: a minimum point of $f(x)$, $x^{*}$.</p>

<p>Step1: load $x_0$, and set $k = 0$, $H_0 = I$.</p>

<p>Step2: calculate $f(x_k)$</p>

<p>Step3: calculate $g_k = g(x_k)$, if $\parallel g_k\parallel \lt \epsilon$, stop iteration and let $x^* = x_k$; otherwise, continue.</p>

<p>Step4: let $x_{k+1} = x_k - H_k g_k$, calculate $f(x_{k+1})$.</p>

<p>Step5: if $\parallel f(x_{k+1}) - f(x_{k})\parallel \lt \epsilon$ or $\parallel x_{k+1} - x_{k}\parallel \lt \epsilon$, stop iteration and let $x^* = x_{k+1}$; otherwise, continue.</p>

<p>Step6: calculate $g_{k+1} = g(x_{k+1})$, if $\parallel g_{k+1}\parallel \lt \epsilon$, stop iteration and let $x^* = x_{l+1}; otherwise, $calculate $\Delta x_k = x_{k+1} - x_k$, $\Delta g_k = g_{k+1} - g_k$.</p>

<p>Step7: let $H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^T}{\Delta x_k^T \Delta g_k} - \frac{H_k \Delta g_k \Delta g_k^T H_k}{\Delta g_k^T H_k \Delta g_k}$, let $k = k+1$</p>

<p>Step8: if $k = k_{max}$, the algorithm doesn’t converge, print “Does not converge, calculation failed.”; otherwise, return to Step3.</p>

<h3 id="4-code">(4) Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">DFP_Quasi_Newton</span><span class="p">(</span><span class="n">f_x</span><span class="p">,</span> <span class="n">g_x</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">k_max</span> <span class="o">=</span> <span class="mf">1e5</span><span class="p">):</span>
    <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_0</span>
    <span class="n">H_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">g_k</span> <span class="o">=</span> <span class="n">g_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x_k</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_k_new</span> <span class="o">=</span> <span class="n">x_k</span> <span class="o">-</span> <span class="n">H_k</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">g_k</span><span class="p">)</span>
            
            <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">f_x</span><span class="p">(</span><span class="n">x_k_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">f_x</span><span class="p">(</span><span class="n">x_k</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_k_new</span> <span class="o">-</span> <span class="n">x_k</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">x_k_new</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
                <span class="k">break</span>
                
            <span class="k">else</span><span class="p">:</span>
                <span class="n">g_k_new</span> <span class="o">=</span> <span class="n">g_x</span><span class="p">(</span><span class="n">x_k_new</span><span class="p">)</span>
        
                <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_k_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">):</span>
                    <span class="k">return</span> <span class="p">(</span><span class="n">x_k_new</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
                    <span class="k">break</span>
                
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">delta_x_k</span> <span class="o">=</span> <span class="n">x_k_new</span> <span class="o">-</span> <span class="n">x_k</span>
                    <span class="n">delta_g_k</span> <span class="o">=</span> <span class="n">g_k_new</span> <span class="o">-</span> <span class="n">g_k</span>
                    <span class="n">H_k</span> <span class="o">=</span> <span class="n">H_k</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_x_k</span><span class="p">,</span> <span class="n">delta_x_k</span><span class="p">)</span> <span class="o">/</span> <span class="n">delta_x_k</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta_g_k</span><span class="p">)</span> <span class="o">-</span> <span class="n">H_k</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_g_k</span><span class="p">,</span> <span class="n">delta_g_k</span><span class="p">)).</span><span class="n">dot</span><span class="p">(</span><span class="n">H_k</span><span class="p">)</span> <span class="o">/</span> <span class="n">delta_g_k</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H_k</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">delta_g_k</span><span class="p">)</span>
                    <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_k_new</span>

                <span class="k">if</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="n">k_max</span><span class="p">):</span>
                    <span class="k">return</span> <span class="s">"Does not converge, calculation failed."</span>
</code></pre></div></div>

<p>Let $f(X) = (4 − x_2)^3 + (x_1 + 4x_2)^2$.</p>

<p>Then $g(x) = \nabla f(x) = (2(x_1 + 4x_2), -3(4 - x_2)^2 + 8(x_1 + 4x_2))^T$.</p>

<p>Let the starting point be $x_0 = (2,1)^T$, we’ll use Newton’s method and DFP Quasi-Newton method to find a minimum point of $f(x)$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">f_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">((</span><span class="mi">4</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">x_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">g_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">x_2</span><span class="p">),</span> <span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">8</span><span class="o">*</span><span class="p">(</span><span class="n">x_1</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">x_2</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">H_test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">32</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">Newton</span><span class="p">(</span><span class="n">f_test</span><span class="p">,</span> <span class="n">g_test</span><span class="p">,</span> <span class="n">H_test</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">DFP_Quasi_Newton</span><span class="p">(</span><span class="n">f_test</span><span class="p">,</span> <span class="n">g_test</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([-15.9765625 ,   3.99414062]), 8)
(array([-15.9630806,   3.9907569]), 14)
</code></pre></div></div>

<p>As we can see, using DFP Quasi-Newton method is slower than regular Newton’s method, however it’s more stable.</p>

<p>Note: (-16,4) is only a local minimum ($gradient = 0$). $f(x)$ has no global minimum point, since when $x_1 = -4x_2$, and $x_2 \rightarrow + \infty$, $f(x) \rightarrow - \infty$.</p>

<h2 id="references">References:</h2>

<p>1.统计学习方法（第2版）- 李航</p>

<p>2.最优化方法 - 杨庆之</p>

<p>3.Hessian Matrix - Wikipedia</p>

<p>4.Newton’s method - Wikipedia</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/Rymoon/rymoon.github.io">rymoon.github.io</a> is maintained by <a href="https://github.com/Rymoon">Rymoon</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/staff.html" >Staff</a>
  
</nav>
  </body>
</html>
